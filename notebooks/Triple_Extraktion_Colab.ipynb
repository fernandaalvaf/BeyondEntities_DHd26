{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69683b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Setup** (einmal ausf√ºhren) { display-mode: \"form\" }\n",
    "#@markdown Installiert ben√∂tigte Pakete und l√§dt den Code.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Pakete installieren\n",
    "!pip install -q pyyaml requests plotly networkx kaleido\n",
    "\n",
    "# Arbeitsverzeichnis erstellen\n",
    "!mkdir -p /content/triple-colab/src\n",
    "!mkdir -p /content/triple-colab/output_json\n",
    "!mkdir -p /content/triple-colab/logs\n",
    "\n",
    "# Python-Path erweitern\n",
    "if '/content/triple-colab/src' not in sys.path:\n",
    "    sys.path.insert(0, '/content/triple-colab/src')\n",
    "\n",
    "# file_client.py erstellen\n",
    "file_client_code = '''\"\"\"Datei-Client f√ºr XML/TXT-Dateien mit TEI-Optimierung.\"\"\"\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FileClient:\n",
    "    \"\"\"Client zum Lesen von XML/TXT-Dateien mit TEI-Optimierung.\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    def read_content(self) -> Optional[str]:\n",
    "        \"\"\"Liest den Dateiinhalt und extrahiert relevanten Text.\"\"\"\n",
    "        try:\n",
    "            if self.file_extension == '.xml':\n",
    "                return self._read_xml()\n",
    "            elif self.file_extension == '.txt':\n",
    "                return self._read_txt()\n",
    "            else:\n",
    "                logger.warning(f\"Nicht unterst√ºtztes Dateiformat: {self.file_extension}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Lesen von {self.file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _read_xml(self) -> Optional[str]:\n",
    "        \"\"\"Liest XML-Datei und extrahiert TEI-Body-Text.\"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(self.file_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # TEI-Namespace behandeln\n",
    "            namespaces = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "            \n",
    "            # Versuche TEI-Body zu finden\n",
    "            body = root.find('.//tei:body', namespaces)\n",
    "            if body is not None:\n",
    "                return self._extract_text_from_element(body)\n",
    "            \n",
    "            # Fallback: body ohne Namespace\n",
    "            body = root.find('.//body')\n",
    "            if body is not None:\n",
    "                return self._extract_text_from_element(body)\n",
    "            \n",
    "            # Letzter Fallback: gesamter Text\n",
    "            return self._extract_text_from_element(root)\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logger.error(f\"XML-Parse-Fehler in {self.file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _read_txt(self) -> Optional[str]:\n",
    "        \"\"\"Liest TXT-Datei.\"\"\"\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def _extract_text_from_element(self, element) -> str:\n",
    "        \"\"\"Extrahiert rekursiv Text aus XML-Element.\"\"\"\n",
    "        texts = []\n",
    "        \n",
    "        if element.text:\n",
    "            texts.append(element.text.strip())\n",
    "        \n",
    "        for child in element:\n",
    "            texts.append(self._extract_text_from_element(child))\n",
    "            if child.tail:\n",
    "                texts.append(child.tail.strip())\n",
    "        \n",
    "        return ' '.join(filter(None, texts))\n",
    "'''\n",
    "\n",
    "with open('/content/triple-colab/src/file_client.py', 'w') as f:\n",
    "    f.write(file_client_code)\n",
    "\n",
    "# openwebui_client.py erstellen\n",
    "openwebui_client_code = '''\"\"\"OpenWebUI-kompatibler Client mit Gemini/OpenAI-Support.\"\"\"\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OpenWebUIClient:\n",
    "    \"\"\"Client f√ºr OpenAI-kompatible APIs (inkl. Gemini).\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, base_url: str, model: str, temperature: float = 0.1):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 2\n",
    "    \n",
    "    def generate_response(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"Generiert Antwort mit automatischem Retry.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                if 'generativelanguage.googleapis.com' in self.base_url:\n",
    "                    return self._call_gemini_api(prompt, system_prompt)\n",
    "                else:\n",
    "                    return self._call_openai_api(prompt, system_prompt)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Versuch {attempt + 1}/{self.max_retries} fehlgeschlagen: {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (attempt + 1))\n",
    "                else:\n",
    "                    logger.error(f\"Alle Versuche fehlgeschlagen: {e}\")\n",
    "                    return None\n",
    "    \n",
    "    def _call_gemini_api(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"Ruft Gemini API auf.\"\"\"\n",
    "        url = f\"{self.base_url}:generateContent?key={self.api_key}\"\n",
    "        \n",
    "        payload = {\n",
    "            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "            \"generationConfig\": {\"temperature\": self.temperature}\n",
    "        }\n",
    "        \n",
    "        if system_prompt:\n",
    "            payload[\"systemInstruction\"] = {\"parts\": [{\"text\": system_prompt}]}\n",
    "        \n",
    "        response = requests.post(url, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        return data['candidates'][0]['content']['parts'][0]['text']\n",
    "    \n",
    "    def _call_openai_api(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"Ruft OpenAI-kompatible API auf.\"\"\"\n",
    "        url = f\"{self.base_url}/chat/completions\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": self.temperature\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        return data['choices'][0]['message']['content']\n",
    "'''\n",
    "\n",
    "with open('/content/triple-colab/src/openwebui_client.py', 'w') as f:\n",
    "    f.write(openwebui_client_code)\n",
    "\n",
    "# config_loader.py erstellen\n",
    "config_loader_code = '''\"\"\"Einfacher Config-Loader f√ºr YAML.\"\"\"\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "\n",
    "def load_config(config_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"L√§dt YAML-Konfiguration.\"\"\"\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "'''\n",
    "\n",
    "with open('/content/triple-colab/src/config_loader.py', 'w') as f:\n",
    "    f.write(config_loader_code)\n",
    "\n",
    "print(\"‚úì Setup abgeschlossen!\")\n",
    "print(\"‚úì Pakete installiert\")\n",
    "print(\"‚úì Code-Module erstellt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Setup** (einmal ausf√ºhren) { display-mode: \"form\" }\n",
    "#@markdown Installiert ben√∂tigte Pakete und l√§dt den Code.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Pakete installieren\n",
    "!pip install -q pyyaml requests plotly networkx kaleido\n",
    "\n",
    "# Arbeitsverzeichnis erstellen\n",
    "!mkdir -p /content/triple-colab/src\n",
    "!mkdir -p /content/triple-colab/output_json\n",
    "!mkdir -p /content/triple-colab/logs\n",
    "\n",
    "# Python-Path erweitern\n",
    "if '/content/triple-colab/src' not in sys.path:\n",
    "    sys.path.insert(0, '/content/triple-colab/src')\n",
    "\n",
    "# file_client.py erstellen\n",
    "file_client_code = '''\"\"\"Datei-Client f√ºr XML/TXT-Dateien mit TEI-Optimierung.\"\"\"\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FileClient:\n",
    "    \"\"\"Client zum Lesen von XML/TXT-Dateien mit TEI-Optimierung.\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    def read_content(self) -> Optional[str]:\n",
    "        \"\"\"Liest den Dateiinhalt und extrahiert relevanten Text.\"\"\"\n",
    "        try:\n",
    "            if self.file_extension == \".xml\":\n",
    "                return self._read_xml()\n",
    "            elif self.file_extension == \".txt\":\n",
    "                return self._read_txt()\n",
    "            else:\n",
    "                logger.warning(f\"Nicht unterst√ºtztes Dateiformat: {self.file_extension}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler beim Lesen von {self.file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _read_xml(self) -> Optional[str]:\n",
    "        \"\"\"Liest XML-Datei und extrahiert TEI-Body-Text.\"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(self.file_path)\n",
    "            root = tree.getroot()\n",
    "            namespaces = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
    "            body = root.find(\".//tei:body\", namespaces)\n",
    "            if body is not None:\n",
    "                return self._extract_text_from_element(body)\n",
    "            body = root.find(\".//body\")\n",
    "            if body is not None:\n",
    "                return self._extract_text_from_element(body)\n",
    "            return self._extract_text_from_element(root)\n",
    "        except ET.ParseError as e:\n",
    "            logger.error(f\"XML-Parse-Fehler in {self.file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _read_txt(self) -> Optional[str]:\n",
    "        \"\"\"Liest TXT-Datei.\"\"\"\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def _extract_text_from_element(self, element) -> str:\n",
    "        \"\"\"Extrahiert rekursiv Text aus XML-Element.\"\"\"\n",
    "        texts = []\n",
    "        if element.text:\n",
    "            texts.append(element.text.strip())\n",
    "        for child in element:\n",
    "            texts.append(self._extract_text_from_element(child))\n",
    "            if child.tail:\n",
    "                texts.append(child.tail.strip())\n",
    "        return \" \".join(filter(None, texts))\n",
    "'''\n",
    "\n",
    "with open('/content/triple-colab/src/file_client.py', 'w') as f:\n",
    "    f.write(file_client_code)\n",
    "\n",
    "# openwebui_client.py erstellen\n",
    "openwebui_client_code = '''\"\"\"OpenWebUI-kompatibler Client mit Gemini/OpenAI-Support.\"\"\"\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OpenWebUIClient:\n",
    "    \"\"\"Client f√ºr OpenAI-kompatible APIs (inkl. Gemini).\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, base_url: str, model: str, temperature: float = 0.1):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 2\n",
    "    \n",
    "    def generate_response(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"Generiert Antwort mit automatischem Retry.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                if \"generativelanguage.googleapis.com\" in self.base_url:\n",
    "                    return self._call_gemini_api(prompt, system_prompt)\n",
    "                else:\n",
    "                    return self._call_openai_api(prompt, system_prompt)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Versuch {attempt + 1}/{self.max_retries} fehlgeschlagen: {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay * (attempt + 1))\n",
    "                else:\n",
    "                    logger.error(f\"Alle Versuche fehlgeschlagen: {e}\")\n",
    "                    return None\n",
    "    \n",
    "    def _call_gemini_api(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"Ruft Gemini API auf.\"\"\"\n",
    "        url = f\"{self.base_url}:generateContent?key={self.api_key}\"\n",
    "        payload = {\n",
    "            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
    "            \"generationConfig\": {\"temperature\": self.temperature}\n",
    "        }\n",
    "        if system_prompt:\n",
    "            payload[\"systemInstruction\"] = {\"parts\": [{\"text\": system_prompt}]}\n",
    "        response = requests.post(url, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "    \n",
    "    def _call_openai_api(self, prompt: str, system_prompt: Optional[str] = None) -> Optional[str]:\n",
    "        \"\"\"Ruft OpenAI-kompatible API auf.\"\"\"\n",
    "        url = f\"{self.base_url}/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": self.temperature\n",
    "        }\n",
    "        response = requests.post(url, headers=headers, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "'''\n",
    "\n",
    "with open('/content/triple-colab/src/openwebui_client.py', 'w') as f:\n",
    "    f.write(openwebui_client_code)\n",
    "\n",
    "print(\"‚úì Setup abgeschlossen!\")\n",
    "print(\"‚úì Pakete installiert\")\n",
    "print(\"‚úì Code-Module erstellt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f367c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **API-Konfiguration** { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ### W√§hle deinen API-Provider:\n",
    "api_provider = \"Gemini (Google)\" #@param [\"Gemini (Google)\", \"ChatAI (AcademicCloud)\", \"Eigene OpenAI-API\"]\n",
    "\n",
    "#@markdown ### API-Schl√ºssel:\n",
    "#@markdown **Wichtig:** Gib niemals deinen API-Schl√ºssel in √∂ffentlichen Notebooks weiter! L√∂sche ihn vor dem Teilen.\n",
    "api_key = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Eigenes Modell (optional):\n",
    "#@markdown Leer lassen f√ºr Standard-Modell des Providers\n",
    "custom_model = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Detailgrad der Extraktion:\n",
    "#@markdown Wie ausf√ºhrlich sollen die Triples sein? (1=nur Kernaussagen, 10=sehr detailliert)\n",
    "granularity = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "\n",
    "#@markdown ### Erweiterte Optionen:\n",
    "#@markdown Datei-Metadaten in Ergebnissen speichern (Dateiname, Datum, etc.):\n",
    "save_file_metadata = True #@param {type:\"boolean\"}\n",
    "#@markdown Debug-Modus: Zeige KI-Antworten bei Fehlern (hilft bei Problemanalyse):\n",
    "show_debug_output = False #@param {type:\"boolean\"}\n",
    "\n",
    "# API-Schl√ºssel validieren\n",
    "if not api_key or len(api_key.strip()) < 10:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Ung√ºltiger API-Schl√ºssel!\\n\\n\"\n",
    "        \"Bitte gib einen g√ºltigen API-Schl√ºssel ein.\\n\\n\"\n",
    "        \"API-Schl√ºssel erhalten:\\n\"\n",
    "        \"‚Ä¢ Gemini: https://aistudio.google.com/apikey\\n\"\n",
    "        \"‚Ä¢ ChatAI: https://chat-ai.academiccloud.de/\\n\"\n",
    "        \"‚Ä¢ OpenAI: https://platform.openai.com/api-keys\"\n",
    "    )\n",
    "\n",
    "# Konfiguration basierend auf Provider\n",
    "if api_provider == \"Gemini (Google)\":\n",
    "    if custom_model:\n",
    "        selected_model = custom_model\n",
    "        base_url = f\"https://generativelanguage.googleapis.com/v1beta/models/{custom_model}\"\n",
    "    else:\n",
    "        selected_model = \"gemini-2.0-flash\"\n",
    "        base_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash\"\n",
    "elif api_provider == \"ChatAI (AcademicCloud)\":\n",
    "    selected_model = custom_model if custom_model else \"llama-3.3-70b-instruct\"\n",
    "    base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "else:\n",
    "    selected_model = custom_model if custom_model else \"gpt-4o-mini\"\n",
    "    base_url = \"https://api.openai.com/v1\"\n",
    "\n",
    "# Konfiguration speichern\n",
    "selected_config = {\n",
    "    'provider': api_provider,\n",
    "    'api_key': api_key,\n",
    "    'base_url': base_url,\n",
    "    'model': selected_model,\n",
    "    'temperature': 0.1,\n",
    "    'granularity': granularity,\n",
    "    'include_metadata': save_file_metadata,\n",
    "    'verbose': show_debug_output\n",
    "}\n",
    "\n",
    "print(f\"‚úì Konfiguration gespeichert\")\n",
    "print(f\"  Provider: {api_provider}\")\n",
    "print(f\"  Modell: {selected_model}\")\n",
    "print(f\"  Detailgrad: {granularity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **System-Prompt bearbeiten** (optional) { display-mode: \"form\" }\n",
    "#@markdown Hier kannst du den System-Prompt anpassen. Nur relevant wenn \"Eigenen Prompt verwenden\" aktiviert ist.\n",
    "\n",
    "#@markdown ### Eigenen System-Prompt verwenden:\n",
    "use_custom_prompt = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### System-Prompt:\n",
    "CUSTOM_PROMPT = \"\"\"Du bist ein Experte f√ºr die Analyse historischer Briefe und die Extraktion von Wissensgraphen.\n",
    "\n",
    "Deine Aufgabe ist es, aus dem gegebenen Brieftext semantische Triples im Format (Subjekt, Pr√§dikat, Objekt) zu extrahieren.\n",
    "\n",
    "Beachte dabei:\n",
    "- Extrahiere nur faktische Beziehungen, keine Interpretationen\n",
    "- Verwende klare, pr√§zise Pr√§dikate\n",
    "- Normalisiere Entit√§ten (z.B. \"J. W. v. Goethe\" -> \"Johann Wolfgang von Goethe\")\n",
    "- Ber√ºcksichtige historischen Kontext\n",
    "- Extrahiere sowohl explizite als auch implizite Beziehungen\n",
    "\n",
    "Antworte ausschlie√ülich im JSON-Format mit folgendem Schema:\n",
    "{\n",
    "  \"triples\": [\n",
    "    {\"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}\n",
    "  ]\n",
    "}\"\"\" #@param {type:\"string\"}\n",
    "\n",
    "if use_custom_prompt:\n",
    "    print(\"‚úì Eigener System-Prompt aktiviert\")\n",
    "    print(f\"  L√§nge: {len(CUSTOM_PROMPT)} Zeichen\")\n",
    "else:\n",
    "    print(\"‚Ñπ Standard-Prompt wird verwendet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88338e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Dateien hochladen** { display-mode: \"form\" }\n",
    "#@markdown Klicke auf \"Dateien ausw√§hlen\" und w√§hle deine XML-Dateien oder ein ZIP-Archiv.\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Upload-Verzeichnis vorbereiten\n",
    "upload_dir = '/content/triple-colab/uploads'\n",
    "if os.path.exists(upload_dir):\n",
    "    shutil.rmtree(upload_dir)\n",
    "os.makedirs(upload_dir)\n",
    "\n",
    "print(\"Bitte w√§hle deine Dateien aus...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Hochgeladene Dateien verarbeiten\n",
    "uploaded_files = []\n",
    "for filename, content in uploaded.items():\n",
    "    filepath = os.path.join(upload_dir, filename)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    # ZIP-Archive entpacken\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"Entpacke {filename}...\")\n",
    "        with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "            zip_ref.extractall(upload_dir)\n",
    "        os.remove(filepath)\n",
    "\n",
    "# XML-Dateien sammeln\n",
    "for root, dirs, files_in_dir in os.walk(upload_dir):\n",
    "    for file in files_in_dir:\n",
    "        if file.endswith('.xml'):\n",
    "            uploaded_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"\\n‚úì {len(uploaded_files)} XML-Datei(en) gefunden:\")\n",
    "for f in uploaded_files[:5]:\n",
    "    print(f\"  ‚Ä¢ {os.path.basename(f)}\")\n",
    "if len(uploaded_files) > 5:\n",
    "    print(f\"  ... und {len(uploaded_files) - 5} weitere\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Verarbeitung starten** { display-mode: \"form\" }\n",
    "#@markdown Startet die Triple-Extraktion f√ºr alle hochgeladenen Dateien.\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from file_client import FileClient\n",
    "from openwebui_client import OpenWebUIClient\n",
    "\n",
    "# Validierung\n",
    "if 'selected_config' not in globals():\n",
    "    raise ValueError(\n",
    "        \"‚ùå Keine API-Konfiguration gefunden!\\n\\n\"\n",
    "        \"Bitte f√ºhre zuerst die Zelle 'API-Konfiguration' aus.\"\n",
    "    )\n",
    "\n",
    "if 'api_key' not in globals() or not api_key:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Kein API-Schl√ºssel gefunden!\\n\\n\"\n",
    "        \"Bitte f√ºhre die Zelle 'API-Konfiguration' aus und gib deinen API-Schl√ºssel ein.\"\n",
    "    )\n",
    "\n",
    "if 'uploaded_files' not in globals() or not uploaded_files:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Keine Dateien hochgeladen!\\n\\n\"\n",
    "        \"Bitte f√ºhre zuerst die Zelle 'Dateien hochladen' aus.\"\n",
    "    )\n",
    "\n",
    "# Client initialisieren\n",
    "client = OpenWebUIClient(\n",
    "    api_key=selected_config['api_key'],\n",
    "    base_url=selected_config['base_url'],\n",
    "    model=selected_config['model'],\n",
    "    temperature=selected_config['temperature']\n",
    ")\n",
    "\n",
    "# System-Prompt\n",
    "system_prompt = CUSTOM_PROMPT if ('use_custom_prompt' in globals() and use_custom_prompt) else None\n",
    "\n",
    "# Prompt-Template\n",
    "def create_prompt(text, granularity):\n",
    "    return f\"\"\"Extrahiere semantische Triples aus folgendem historischen Brieftext.\n",
    "\n",
    "Granularit√§t: {granularity}/10 (1=grob, 10=sehr detailliert)\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Antworte NUR mit g√ºltigem JSON im Format:\n",
    "{{\n",
    "  \"triples\": [\n",
    "    {{\"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "\n",
    "# Verarbeitung\n",
    "results = []\n",
    "output_dir = '/content/triple-colab/output_json'\n",
    "\n",
    "print(f\"Verarbeite {len(uploaded_files)} Datei(en)...\\n\")\n",
    "\n",
    "for i, filepath in enumerate(uploaded_files, 1):\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"[{i}/{len(uploaded_files)}] {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Text einlesen\n",
    "        file_client = FileClient(filepath)\n",
    "        text = file_client.read_content()\n",
    "        \n",
    "        if not text:\n",
    "            print(f\"  ‚ö† Konnte Text nicht extrahieren\")\n",
    "            continue\n",
    "        \n",
    "        # API-Anfrage\n",
    "        prompt = create_prompt(text, selected_config['granularity'])\n",
    "        response = client.generate_response(prompt, system_prompt)\n",
    "        \n",
    "        if not response:\n",
    "            print(f\"  ‚ùå Keine Antwort erhalten\")\n",
    "            continue\n",
    "        \n",
    "        # JSON parsen\n",
    "        try:\n",
    "            clean_response = response.strip()\n",
    "            if clean_response.startswith('```'):\n",
    "                clean_response = '\\n'.join(clean_response.split('\\n')[1:-1])\n",
    "            if clean_response.startswith('json'):\n",
    "                clean_response = '\\n'.join(clean_response.split('\\n')[1:])\n",
    "            \n",
    "            data = json.loads(clean_response)\n",
    "            triple_count = len(data.get('triples', []))\n",
    "            \n",
    "            # Ergebnis speichern\n",
    "            output_filename = os.path.splitext(filename)[0] + '.json'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            results.append({\n",
    "                'filename': filename,\n",
    "                'triples': data.get('triples', []),\n",
    "                'output_path': output_path\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚úì {triple_count} Triples extrahiert\")\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  ‚ùå JSON-Parse-Fehler: {e}\")\n",
    "            if selected_config.get('verbose', False):\n",
    "                print(f\"  Response: {response[:200]}...\")\n",
    "            continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Fehler: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úì Verarbeitung abgeschlossen!\")\n",
    "print(f\"  {len(results)} von {len(uploaded_files)} Dateien erfolgreich verarbeitet\")\n",
    "total_triples = sum(len(r['triples']) for r in results)\n",
    "print(f\"  {total_triples} Triples gesamt extrahiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf617d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Ergebnis visualisieren** { display-mode: \"form\" }\n",
    "#@markdown Zeigt den Wissensgraphen f√ºr das zuletzt verarbeitete Ergebnis.\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "\n",
    "if 'results' not in globals() or not results:\n",
    "    print(\"‚ùå Keine Ergebnisse vorhanden. Bitte f√ºhre zuerst die Verarbeitung aus.\")\n",
    "else:\n",
    "    last_result = results[-1]\n",
    "    triples = last_result['triples']\n",
    "    \n",
    "    if not triples:\n",
    "        print(\"‚ö† Keine Triples zum Visualisieren gefunden.\")\n",
    "    else:\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        for triple in triples:\n",
    "            subj = triple.get('subject', '')\n",
    "            pred = triple.get('predicate', '')\n",
    "            obj = triple.get('object', '')\n",
    "            G.add_edge(subj, obj, label=pred)\n",
    "        \n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        edge_trace = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_trace.append(\n",
    "                go.Scatter(\n",
    "                    x=[x0, x1, None],\n",
    "                    y=[y0, y1, None],\n",
    "                    mode='lines',\n",
    "                    line=dict(width=1, color='#888'),\n",
    "                    hoverinfo='none'\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        node_text = []\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            x, y = pos[node]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "            node_text.append(node)\n",
    "        \n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x,\n",
    "            y=node_y,\n",
    "            mode='markers+text',\n",
    "            text=node_text,\n",
    "            textposition='top center',\n",
    "            hoverinfo='text',\n",
    "            marker=dict(\n",
    "                size=20,\n",
    "                color='lightblue',\n",
    "                line=dict(width=2, color='darkblue')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig = go.Figure(\n",
    "            data=edge_trace + [node_trace],\n",
    "            layout=go.Layout(\n",
    "                title=f\"Wissensgraph: {last_result['filename']}\",\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=0, l=0, r=0, t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                height=600\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        print(f\"\\nGraph mit {len(G.nodes())} Entit√§ten und {len(G.edges())} Beziehungen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fff0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Ergebnisse als ZIP herunterladen** { display-mode: \"form\" }\n",
    "#@markdown L√§dt alle JSON-Ergebnisse als ZIP-Archiv herunter.\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "if 'results' not in globals() or not results:\n",
    "    print(\"‚ùå Keine Ergebnisse vorhanden. Bitte f√ºhre zuerst die Verarbeitung aus.\")\n",
    "else:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    zip_filename = f'triple_extraction_results_{timestamp}.zip'\n",
    "    zip_path = f'/content/{zip_filename}'\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        for result in results:\n",
    "            output_path = result['output_path']\n",
    "            arcname = os.path.basename(output_path)\n",
    "            zipf.write(output_path, arcname)\n",
    "    \n",
    "    print(f\"Lade {zip_filename} herunter...\")\n",
    "    files.download(zip_path)\n",
    "    print(f\"‚úì Download gestartet ({len(results)} Dateien)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c278fd",
   "metadata": {},
   "source": [
    "## Hilfe & Troubleshooting\n",
    "\n",
    "| Problem | L√∂sung |\n",
    "|---------|--------|\n",
    "| **NameError: api_key not defined** | F√ºhre die Zelle \"API-Konfiguration\" aus |\n",
    "| **Ung√ºltiger API-Schl√ºssel** | Pr√ºfe ob der Schl√ºssel korrekt kopiert wurde |\n",
    "| **Rate Limit Error** | Warte kurz und versuche es erneut |\n",
    "| **Timeout** | Versuche es mit weniger oder kleineren Dateien |\n",
    "| **Keine Triples extrahiert** | Erh√∂he den Detailgrad oder passe den Prompt an |\n",
    "| **JSON Parse Error** | Aktiviere \"Debug-Modus\" in der Konfiguration |\n",
    "\n",
    "**API-Schl√ºssel erhalten:**\n",
    "- Gemini: https://aistudio.google.com/apikey\n",
    "- ChatAI: https://chat-ai.academiccloud.de/\n",
    "- OpenAI: https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba252aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **API-Konfiguration** { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ### W√§hle deinen API-Provider:\n",
    "api_provider = \"Gemini (Google)\" #@param [\"Gemini (Google)\", \"ChatAI (AcademicCloud)\", \"Eigene OpenAI-API\"]\n",
    "\n",
    "#@markdown ### API-Schl√ºssel:\n",
    "#@markdown **‚ö†Ô∏è Wichtig:** Gib niemals deinen API-Schl√ºssel in √∂ffentlichen Notebooks weiter! L√∂sche ihn vor dem Teilen.\n",
    "api_key = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Eigenes Modell (optional):\n",
    "#@markdown Leer lassen f√ºr Standard-Modell des Providers\n",
    "custom_model = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Detailgrad der Extraktion:\n",
    "#@markdown Wie ausf√ºhrlich sollen die Triples sein? (1=nur Kernaussagen, 10=sehr detailliert)\n",
    "granularity = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "\n",
    "#@markdown ### Erweiterte Optionen:\n",
    "#@markdown Datei-Metadaten in Ergebnissen speichern (Dateiname, Datum, etc.):\n",
    "save_file_metadata = True #@param {type:\"boolean\"}\n",
    "#@markdown Debug-Modus: Zeige KI-Antworten bei Fehlern (hilft bei Problemanalyse):\n",
    "show_debug_output = False #@param {type:\"boolean\"}\n",
    "\n",
    "# API-Schl√ºssel validieren\n",
    "if not api_key or len(api_key.strip()) < 10:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Ung√ºltiger API-Schl√ºssel!\\n\\n\"\n",
    "        \"Bitte gib einen g√ºltigen API-Schl√ºssel ein.\\n\\n\"\n",
    "        \"API-Schl√ºssel erhalten:\\n\"\n",
    "        \"‚Ä¢ Gemini: https://aistudio.google.com/apikey\\n\"\n",
    "        \"‚Ä¢ ChatAI: https://chat-ai.academiccloud.de/\\n\"\n",
    "        \"‚Ä¢ OpenAI: https://platform.openai.com/api-keys\"\n",
    "    )\n",
    "\n",
    "# Konfiguration basierend auf Provider\n",
    "if api_provider == \"Gemini (Google)\":\n",
    "    if custom_model:\n",
    "        selected_model = custom_model\n",
    "        base_url = f\"https://generativelanguage.googleapis.com/v1beta/models/{custom_model}\"\n",
    "    else:\n",
    "        selected_model = \"gemini-2.0-flash\"\n",
    "        base_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash\"\n",
    "elif api_provider == \"ChatAI (AcademicCloud)\":\n",
    "    selected_model = custom_model if custom_model else \"llama-3.3-70b-instruct\"\n",
    "    base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "else:  # Eigene OpenAI-API\n",
    "    selected_model = custom_model if custom_model else \"gpt-4o-mini\"\n",
    "    base_url = \"https://api.openai.com/v1\"\n",
    "\n",
    "# Konfiguration speichern\n",
    "selected_config = {\n",
    "    'provider': api_provider,\n",
    "    'api_key': api_key,\n",
    "    'base_url': base_url,\n",
    "    'model': selected_model,\n",
    "    'temperature': 0.1,\n",
    "    'granularity': granularity,\n",
    "    'include_metadata': save_file_metadata,\n",
    "    'verbose': show_debug_output\n",
    "}\n",
    "\n",
    "print(f\"‚úì Konfiguration gespeichert\")\n",
    "print(f\"  Provider: {api_provider}\")\n",
    "print(f\"  Modell: {selected_model}\")\n",
    "print(f\"  Detailgrad: {granularity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe4975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **System-Prompt bearbeiten** (optional) { display-mode: \"form\" }\n",
    "#@markdown Hier kannst du den System-Prompt anpassen. Nur relevant wenn \"use_custom_prompt\" aktiviert ist.\n",
    "\n",
    "#@markdown ### Eigenen System-Prompt verwenden:\n",
    "use_custom_prompt = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### System-Prompt:\n",
    "CUSTOM_PROMPT = \"\"\"Du bist ein Experte f√ºr die Analyse historischer Briefe und die Extraktion von Wissensgraphen.\n",
    "\n",
    "Deine Aufgabe ist es, aus dem gegebenen Brieftext semantische Triples im Format (Subjekt, Pr√§dikat, Objekt) zu extrahieren.\n",
    "\n",
    "Beachte dabei:\n",
    "- Extrahiere nur faktische Beziehungen, keine Interpretationen\n",
    "- Verwende klare, pr√§zise Pr√§dikate\n",
    "- Normalisiere Entit√§ten (z.B. \"J. W. v. Goethe\" ‚Üí \"Johann Wolfgang von Goethe\")\n",
    "- Ber√ºcksichtige historischen Kontext\n",
    "- Extrahiere sowohl explizite als auch implizite Beziehungen\n",
    "\n",
    "Antworte ausschlie√ülich im JSON-Format mit folgendem Schema:\n",
    "{\n",
    "  \"triples\": [\n",
    "    {\"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}\n",
    "  ]\n",
    "}\"\"\" #@param {type:\"string\"}\n",
    "\n",
    "if use_custom_prompt:\n",
    "    print(\"‚úì Eigener System-Prompt aktiviert\")\n",
    "    print(f\"  L√§nge: {len(CUSTOM_PROMPT)} Zeichen\")\n",
    "else:\n",
    "    print(\"‚Ñπ Standard-Prompt wird verwendet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152706d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Dateien hochladen** { display-mode: \"form\" }\n",
    "#@markdown Klicke auf \"Dateien ausw√§hlen\" und w√§hle deine XML-Dateien oder ein ZIP-Archiv.\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Upload-Verzeichnis vorbereiten\n",
    "upload_dir = '/content/triple-colab/uploads'\n",
    "if os.path.exists(upload_dir):\n",
    "    shutil.rmtree(upload_dir)\n",
    "os.makedirs(upload_dir)\n",
    "\n",
    "print(\"üìÅ Bitte w√§hle deine Dateien aus...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Hochgeladene Dateien verarbeiten\n",
    "uploaded_files = []\n",
    "for filename, content in uploaded.items():\n",
    "    filepath = os.path.join(upload_dir, filename)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    # ZIP-Archive entpacken\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"üì¶ Entpacke {filename}...\")\n",
    "        with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "            zip_ref.extractall(upload_dir)\n",
    "        os.remove(filepath)\n",
    "\n",
    "# XML-Dateien sammeln\n",
    "for root, dirs, files_in_dir in os.walk(upload_dir):\n",
    "    for file in files_in_dir:\n",
    "        if file.endswith('.xml'):\n",
    "            uploaded_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"\\n‚úì {len(uploaded_files)} XML-Datei(en) gefunden:\")\n",
    "for f in uploaded_files[:5]:\n",
    "    print(f\"  ‚Ä¢ {os.path.basename(f)}\")\n",
    "if len(uploaded_files) > 5:\n",
    "    print(f\"  ... und {len(uploaded_files) - 5} weitere\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0789fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Verarbeitung starten** { display-mode: \"form\" }\n",
    "#@markdown Startet die Triple-Extraktion f√ºr alle hochgeladenen Dateien.\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from file_client import FileClient\n",
    "from openwebui_client import OpenWebUIClient\n",
    "\n",
    "# Validierung\n",
    "if 'selected_config' not in globals():\n",
    "    raise ValueError(\n",
    "        \"‚ùå Keine API-Konfiguration gefunden!\\n\\n\"\n",
    "        \"Bitte f√ºhre zuerst die Zelle 'API-Konfiguration' aus.\"\n",
    "    )\n",
    "\n",
    "if 'api_key' not in globals() or not api_key:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Kein API-Schl√ºssel gefunden!\\n\\n\"\n",
    "        \"Bitte f√ºhre die Zelle 'API-Konfiguration' aus und gib deinen API-Schl√ºssel ein.\"\n",
    "    )\n",
    "\n",
    "if 'uploaded_files' not in globals() or not uploaded_files:\n",
    "    raise ValueError(\n",
    "        \"‚ùå Keine Dateien hochgeladen!\\n\\n\"\n",
    "        \"Bitte f√ºhre zuerst die Zelle 'Dateien hochladen' aus.\"\n",
    "    )\n",
    "\n",
    "# Client initialisieren\n",
    "client = OpenWebUIClient(\n",
    "    api_key=selected_config['api_key'],\n",
    "    base_url=selected_config['base_url'],\n",
    "    model=selected_config['model'],\n",
    "    temperature=selected_config['temperature']\n",
    ")\n",
    "\n",
    "# System-Prompt\n",
    "system_prompt = CUSTOM_PROMPT if ('use_custom_prompt' in globals() and use_custom_prompt) else None\n",
    "\n",
    "# Prompt-Template\n",
    "def create_prompt(text: str, granularity: int) -> str:\n",
    "    return f\"\"\"Extrahiere semantische Triples aus folgendem historischen Brieftext.\n",
    "\n",
    "Granularit√§t: {granularity}/10 (1=grob, 10=sehr detailliert)\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Antworte NUR mit g√ºltigem JSON im Format:\n",
    "{{\n",
    "  \"triples\": [\n",
    "    {{\"subject\": \"...\", \"predicate\": \"...\", \"object\": \"...\"}}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "\n",
    "# Verarbeitung\n",
    "results = []\n",
    "output_dir = '/content/triple-colab/output_json'\n",
    "\n",
    "print(f\"üîÑ Verarbeite {len(uploaded_files)} Datei(en)...\\n\")\n",
    "\n",
    "for i, filepath in enumerate(uploaded_files, 1):\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"[{i}/{len(uploaded_files)}] {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Text einlesen\n",
    "        file_client = FileClient(filepath)\n",
    "        text = file_client.read_content()\n",
    "        \n",
    "        if not text:\n",
    "            print(f\"  ‚ö† Konnte Text nicht extrahieren\")\n",
    "            continue\n",
    "        \n",
    "        # API-Anfrage\n",
    "        prompt = create_prompt(text, selected_config['granularity'])\n",
    "        response = client.generate_response(prompt, system_prompt)\n",
    "        \n",
    "        if not response:\n",
    "            print(f\"  ‚ùå Keine Antwort erhalten\")\n",
    "            continue\n",
    "        \n",
    "        # JSON parsen\n",
    "        try:\n",
    "            # Bereinige Response (entferne Markdown-Code-Bl√∂cke)\n",
    "            clean_response = response.strip()\n",
    "            if clean_response.startswith('```'):\n",
    "                clean_response = '\\n'.join(clean_response.split('\\n')[1:-1])\n",
    "            if clean_response.startswith('json'):\n",
    "                clean_response = '\\n'.join(clean_response.split('\\n')[1:])\n",
    "            \n",
    "            data = json.loads(clean_response)\n",
    "            triple_count = len(data.get('triples', []))\n",
    "            \n",
    "            # Ergebnis speichern\n",
    "            output_filename = os.path.splitext(filename)[0] + '.json'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            results.append({\n",
    "                'filename': filename,\n",
    "                'triples': data.get('triples', []),\n",
    "                'output_path': output_path\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚úì {triple_count} Triples extrahiert\")\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  ‚ùå JSON-Parse-Fehler: {e}\")\n",
    "            if selected_config.get('verbose', False):\n",
    "                print(f\"  Response: {response[:200]}...\")\n",
    "            continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Fehler: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úì Verarbeitung abgeschlossen!\")\n",
    "print(f\"  {len(results)} von {len(uploaded_files)} Dateien erfolgreich verarbeitet\")\n",
    "total_triples = sum(len(r['triples']) for r in results)\n",
    "print(f\"  {total_triples} Triples gesamt extrahiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Ergebnis visualisieren** { display-mode: \"form\" }\n",
    "#@markdown Zeigt den Wissensgraphen f√ºr das zuletzt verarbeitete Ergebnis.\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "\n",
    "if 'results' not in globals() or not results:\n",
    "    print(\"‚ùå Keine Ergebnisse vorhanden. Bitte f√ºhre zuerst die Verarbeitung aus.\")\n",
    "else:\n",
    "    # Letztes Ergebnis nehmen\n",
    "    last_result = results[-1]\n",
    "    triples = last_result['triples']\n",
    "    \n",
    "    if not triples:\n",
    "        print(\"‚ö† Keine Triples zum Visualisieren gefunden.\")\n",
    "    else:\n",
    "        # Graph erstellen\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        for triple in triples:\n",
    "            subj = triple.get('subject', '')\n",
    "            pred = triple.get('predicate', '')\n",
    "            obj = triple.get('object', '')\n",
    "            G.add_edge(subj, obj, label=pred)\n",
    "        \n",
    "        # Layout berechnen\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Kanten erstellen\n",
    "        edge_trace = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_trace.append(\n",
    "                go.Scatter(\n",
    "                    x=[x0, x1, None],\n",
    "                    y=[y0, y1, None],\n",
    "                    mode='lines',\n",
    "                    line=dict(width=1, color='#888'),\n",
    "                    hoverinfo='none'\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Knoten erstellen\n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        node_text = []\n",
    "        \n",
    "        for node in G.nodes():\n",
    "            x, y = pos[node]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "            node_text.append(node)\n",
    "        \n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x,\n",
    "            y=node_y,\n",
    "            mode='markers+text',\n",
    "            text=node_text,\n",
    "            textposition='top center',\n",
    "            hoverinfo='text',\n",
    "            marker=dict(\n",
    "                size=20,\n",
    "                color='lightblue',\n",
    "                line=dict(width=2, color='darkblue')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Plot erstellen\n",
    "        fig = go.Figure(\n",
    "            data=edge_trace + [node_trace],\n",
    "            layout=go.Layout(\n",
    "                title=f\"Wissensgraph: {last_result['filename']}\",\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=0, l=0, r=0, t=40),\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                height=600\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "        print(f\"\\nüìä Graph mit {len(G.nodes())} Entit√§ten und {len(G.edges())} Beziehungen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title **Ergebnisse als ZIP herunterladen** { display-mode: \"form\" }\n",
    "#@markdown L√§dt alle JSON-Ergebnisse als ZIP-Archiv herunter.\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "if 'results' not in globals() or not results:\n",
    "    print(\"‚ùå Keine Ergebnisse vorhanden. Bitte f√ºhre zuerst die Verarbeitung aus.\")\n",
    "else:\n",
    "    # ZIP erstellen\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    zip_filename = f'triple_extraction_results_{timestamp}.zip'\n",
    "    zip_path = f'/content/{zip_filename}'\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        for result in results:\n",
    "            output_path = result['output_path']\n",
    "            arcname = os.path.basename(output_path)\n",
    "            zipf.write(output_path, arcname)\n",
    "    \n",
    "    # Download starten\n",
    "    print(f\"üì¶ Lade {zip_filename} herunter...\")\n",
    "    files.download(zip_path)\n",
    "    print(f\"‚úì Download gestartet ({len(results)} Dateien)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f3a478",
   "metadata": {},
   "source": [
    "## Hilfe & Troubleshooting\n",
    "\n",
    "| Problem | L√∂sung |\n",
    "|---------|--------|\n",
    "| **NameError: api_key not defined** | F√ºhre die Zelle \"API-Konfiguration\" aus |\n",
    "| **Ung√ºltiger API-Schl√ºssel** | Pr√ºfe ob der Schl√ºssel korrekt kopiert wurde |\n",
    "| **Rate Limit Error** | Warte kurz und versuche es erneut |\n",
    "| **Timeout** | Versuche es mit weniger oder kleineren Dateien |\n",
    "| **Keine Triples extrahiert** | Erh√∂he die Granularit√§t oder passe den Prompt an |\n",
    "| **JSON Parse Error** | Aktiviere \"verbose_output\" in der Konfiguration |\n",
    "\n",
    "**API-Schl√ºssel erhalten:**\n",
    "- Gemini: https://aistudio.google.com/apikey\n",
    "- ChatAI: https://chat-ai.academiccloud.de/\n",
    "- OpenAI: https://platform.openai.com/api-keys"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
